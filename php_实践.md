### PHP 实践

#### 1.单KEY业务，数据库水平切分架构实践

以“用户中心”为例，介绍“单KEY”类业务，随着数据量的逐步增大，数据库性能显著降低，数据库水平切分相关的架构实践：

- 如何来实施水平切分
- 水平切分后常见的问题
- 典型问题的优化思路及实践

##### 1、用户中心

用户中心是一个非常常见的业务，主要提供==用户注册、登录、信息查询与修改==的服务，其核心元数据为：

User(uid, login_name, passwd, sex, age, nickname, …)

其中：

- uid为用户ID，主键
- login_name, passwd, sex, age, nickname, …等用户属性

数据库设计上，一般来说在业务**初期**，==单库单表==就能够搞定这个需求，典型的架构设计为：

![图片](php_实践.assets/640)

- user-center：用户中心服务，对调用者提供友好的RPC接口
- user-db：对用户进行数据存储

##### 2、用户中心水平切分方法

当**数据量越来越大时**，需要对数据库进行==水平切分==，常见的水平切分算法有**“范围法”和“哈希法”**。

**范围法**，以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

![图片](php_实践.assets/640-20211008112653753)

- user-db1：存储0到1千万的uid数据
- user-db2：存储1到2千万的uid数据

范围法的**优点**是：

- ==切分策略简单==，根据uid，按照范围，user- center很快能够定位到数据在哪个库上
- ==扩容简单==，如果容量不够，只要增加user-db3即可

范围法的**不足**是：

- uid必须要满足==递增的特性==
- ==数据量不均==，新增的user-db3，在初期的数据会比较少
- ==请求量不均==，一般来说，新注册的用户活跃度会比较高，故user-db2往往会比user-db1负载要高，导致服务器利用率不平衡



**哈希法**，也是以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

![图片](php_实践.assets/640-20211008112822156)

- user-db1：存储uid取模得1的uid数据
- user-db2：存储uid取模得0的uid数据

哈希法的**优点**是：

- ==切分策略简单==，根据uid，按照hash，user-center很快能够定位到数据在哪个库上
- ==数据量均衡==，只要uid是均匀的，数据在各个库上的分布一定是均衡的
- ==请求量均衡==，只要uid是均匀的，负载在各个库上的分布一定是均衡的



哈希法的**不足**是：

- ==扩容麻烦==，如果容量不够，要增加一个库，重新hash可能会导致数据迁移，如何平滑的进行数据迁移，是一个需要解决的问题



##### 3、用户中心水平切分后带来的问题

###### 使用uid来进行水平切分之后，整个用户中心的业务访问会遇到什么问题呢？

==对于uid属性上的查询可以直接路由到库==，假设访问uid=124的数据，取模后能够直接定位db-user1：

![图片](php_实践.assets/640-20211008113043604)

==对于非uid属性上的查询，例如login_name属性上的查询，就悲剧了：==

![图片](php_实践.assets/640-20211008113110625)

假设访问login_name=shenjian的数据，由于不知道数据落在哪个库上，往往需要遍历所有库，当分库数量多起来，性能会显著降低。



##### 4、用户中心非uid属性查询需求分析

用户中心非uid属性上经常有两类业务需求：

（1）**用户侧，前台访问**，最典型的有两类需求

​		==用户登录==：通过login_name/phone/email查询用户的实体，1%请求属于这种类型

​		==用户信息查询==：登录之后，通过uid来查询用户的实例，99%请求属这种类型



用户侧的查询基本上是单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高。

 

（2）**运营侧，后台访问**，根据产品、运营需求，访问模式各异，按照==年龄、性别、头像、登陆时间、注册时间来进行查询==。

运营侧的查询基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格。

 

这两类不同的业务需求，应该使用什么样的架构方案来解决呢？



##### 5、用户中心水平切分架构思路

用户中心在数据量较大的情况下，使用uid进行水平切分，对于非uid属性上的查询需求，架构设计的核心思路为：

- 针对**用户侧**，应该采用“==建立非uid属性到uid的映射关系==”的架构方案
- 针对**运营侧**，应该采用“==前台与后台分离==”的架构方案



##### 6、用户中心-用户侧最佳实践

**【索引表法】**

**思路**：uid能直接定位到库，login_name不能直接定位到库，如果通过login_name能查询到uid，问题解决



**解决方案**：

- 建立一个索引表记录login_name->uid的映射关系
- 用login_name来访问时，先通过索引表查询到uid，再定位相应的库
- 索引表属性较少，可以容纳非常多数据，一般不需要分库
- 如果数据量过大，可以通过login_name来分库



潜在**不足**：==多一次数据库查询，性能下降一倍==



**【login_name生成uid】**

**思路**：不进行远程查询，由login_name直接得到uid



**解决方案**：

- 在用户注册时，设计函数login_name生成uid，uid=f(login_name)，按uid分库插入数据
- 用login_name来访问时，先通过函数计算出uid，即uid=f(login_name)再来一遍，由uid路由到对应库



潜在**不足**：==该函数设计需要非常讲究技巧，有uid生成冲突风险==



**【login_name基因融入uid】**

**思路**：不能用login_name生成uid，可以从login_name抽取“基因”，融入uid中

![图片](php_实践.assets/640-20211008113858893)

假设分8库，采用uid%8路由，潜台词是，uid的最后3个bit决定这条数据落在哪个库上，这3个bit就是所谓的“基因”。

**解决方案**：

- 在用户注册时，设计函数login_name生成3bit基因，login_name_gene=f(login_name)，如上图粉色部分
- 同时，生成61bit的全局唯一id，作为用户的标识，如上图绿色部分
- 接着把3bit的login_name_gene也作为uid的一部分，如上图屎黄色部分
- 生成64bit的uid，由id和login_name_gene拼装而成，并按照uid分库插入数据
- 用login_name来访问时，先通过函数由login_name再次复原3bit基因，login_name_gene=f(login_name)，通过login_name_gene%8直接定位到库



##### 7、用户中心-运营侧最佳实践

前台**用户侧**，业务需求基本都是==单行记录==的访问，只要建立非uid属性 login_name / phone / email 到uid的映射关系，就能解决问题。



后台**运营侧**，业务需求各异，基本是==批量分页==的访问，这类访问计算量较大，返回数据量较大，比较消耗数据库性能。

 

如果此时前台业务和后台业务公用一批服务和一个数据库，有可能导致，由于后台的“少数几个请求”的“批量查询”的“低效”访问，导致数据库的cpu偶尔瞬时100%，影响前台正常用户的访问（例如，登录超时）。

![图片](php_实践.assets/640-20211008114320377)

而且，为了满足后台业务各类“奇形怪状”的需求，往往会在数据库上建立各种索引，这些索引占用大量内存，会使得用户侧前台业务uid/login_name上的查询性能与写入性能大幅度降低，处理时间增长。

对于这一类业务，应该采用“==前台与后台分离==”的架构方案：

![图片](php_实践.assets/640-20211008114356420)

用户侧前台业务需求架构依然不变，产品运营侧后台业务需求则**抽取独立的web / service / db 来支持**，==解除系统之间的耦合==，对于“业务复杂”“并发量低”“无需高可用”“能接受一定延时”的后台业务：

- 可以去掉service层，在运营后台web层通过dao直接访问db
- 不需要反向代理，不需要集群冗余
- 不需要访问实时库，可以通过MQ或者线下异步同步数据
- 在数据库非常大的情况下，可以使用更契合大量数据允许接受更高延时的“索引外置”或者“HIVE”的设计方案

![图片](php_实践.assets/640-20211008114516868)



##### 8、总结

将以“用户中心”为典型的“单KEY”类业务，水平切分的架构点，本文做了这样一些介绍。

 

**水平切分方式**：

- 范围法

- 哈希法

  

**水平切分后碰到的问题**：

- 通过uid属性查询能直接定位到库，通过非uid属性查询不能定位到库

  

**非uid属性查询的典型业务**：

- 用户侧，前台访问，单条记录的查询，访问量较大，服务需要高可用，并且对一致性的要求较高

- 运营侧，后台访问，根据产品、运营需求，访问模式各异，基本上是批量分页的查询，由于是内部系统，访问量很低，对可用性的要求不高，对一致性的要求也没这么严格

  

**这两类业务的架构设计思路**：

- 针对用户侧，应该采用“建立非uid属性到uid的映射关系”的架构方案
- 针对运营侧，应该采用“前台与后台分离”的架构方案



**用户前台侧，“建立非uid属性到uid的映射关系”最佳实践**：

- 索引表法：数据库中记录login_name->uid的映射关系
- 缓存映射法：缓存中记录login_name->uid的映射关系
- login_name生成uid
- login_name基因融入uid



**运营后台侧，“前台与后台分离”最佳实践**：

- 前台、后台系统web/service/db分离解耦，避免后台低效查询引发前台查询抖动
- 可以采用数据冗余的设计方式
- 可以采用“外置索引”（例如ES搜索系统）或者“大数据处理”（例如HIVE）来满足后台变态的查询需求



#### 2. 架构设计中常见“反向依赖”与解耦方案

##### 1、缘起

很多公司，技术经常遇到这样的场景：

​	1）硬件升级，要换一台高配机器

​	2）网络重新规划，若干服务器要调整机架

​	3）服务器当机，要重新部署恢复服务

![图片](php_实践.assets/640-20211008115033274)

更具体的，如上图：==数据库换了一个ip，此时往往连接此数据库的上游需要修改配置重启==，如果数据库有很多上游调用方，改配置重启的调用方会很多，每次换ip的成本往往很高，成为大家共性的痛点。

由A的调整（数据库换ip），配合修改和调整的却是BCDE（改配置重启），BCDE内心非常的郁闷：明明换ip的是你，凭什么配合重启的却是我？

根本上，这是一个“架构耦合”的问题，是一个**架构设计上“反向依赖”的问题**，本文将讨论的是架构设计中常见的“反向依赖”的设计，以及对应的优化方案，希望对大伙有所启示。

##### 2、如何寻找不合理“反向依赖”

**方法论**：

==变动方是A，配合方却是BCDE==

（或者说==需求方是A，改动方确是BCDE==）

想想“换IP的是你，配合重启的却是我”更好理解。

如果系统中经常出现了这类情况，就是“反向依赖”的特征，往往架构上有优化的空间。



##### 3、常见的“反向依赖”与优化方案

**【case1：公共库导致耦合】**

![图片](php_实践.assets/640-20211008115317539)

三个服务s1/s2/s3，通过一个公共的库biz.jar来实现一段业务逻辑，s1/s2/s3其实间接通过biz.jar耦合在了一起，==一个业务s1修改一块公共的代码，导致影响其他业务s2/s3==，架构上是不合理的。

**优化方案1：业务垂直拆分**

![图片](php_实践.assets/640-20211008115409941)

如果biz.jar中实现的逻辑“业务特性”很强，可以拆分为biz1.jar/biz2.jar/biz3.jar，来对s1/s2/s3进行解耦。这样的话，==任何业务的改动，影响范围只是自己，不会影响其他人==。

**优化方案2：服务化**

![图片](php_实践.assets/640-20211008115450686)

如果biz.jar中实现的逻辑“业务共性”很强，可以将biz.jar优化为biz.service服务，来对s1/s2/s3进行解耦。服务化之后，兼容性能更好的通过接口自动化回归测试来保证。

基础服务的抽象，本身是一种共性聚焦，是系统解耦常见的方案。



**【case2：服务化不彻底导致耦合】**

![图片](php_实践.assets/640-20211008115613069)

服务化是解决“业务共性”组件库导致系统耦合的常见方案之一，但如果服务化不彻底，service本身也容易成为业务耦合点。

 

典型的服务化不彻底导致的业务耦合的特征是，共性服务中，包含大量“根据不同业务，执行不同个性分支”的代码。

switch (biz-type)

case biz-1 : exec1

case biz-2 : exec2

case biz-3 : exec3

...

在这种架构下，==biz-1/biz-2/biz-3有个性的业务需求，可能导致修改代码的是共性的biz-service，使其成为研发瓶颈==，架构上也是不合理的。

**优化方案：业务特性代码上浮，业务共性代码下沉，彻底解耦**

![图片](php_实践.assets/640-20211008115740002)

把swithc case中业务==特性代码放到业务层实现==，这样biz-1/biz-2/biz-3有个性的业务需求，升级的是自己的业务系统。



**【case3：notify的不合理实现导致的耦合】**

![图片](php_实践.assets/640-20211008115852646)

《[究竟什么时候该使用MQ](http://mp.weixin.qq.com/s?__biz=MjM5ODYxMDA5OQ==&mid=2651960012&idx=1&sn=c6af5c79ecead98daa4d742e5ad20ce5&chksm=bd2d07108a5a8e0624ae6ad95001c4efe09d7ba695f2ddb672064805d771f3f84bee8123b8a6&scene=21#wechat_redirect)》一文中有一类业务场景，==消息发送方不关注消息接收方的执行结果==，如果采用调用的方式来实现通知，会==导消息发送方和消息接收方耦合==。

 ==如何新增消息接收方biz-4，会发现修改代码的是消息发送方==，新增一个对biz-4的调用，极不合理。

**优化方案：通过MQ实现解耦**

![图片](php_实践.assets/640-20211008120105122)

消息发送方upper将消息发布给MQ，消息接收方从MQ去订阅，==任何新增对消息的消费，upper都不需要修改代码==。



**【case4：配置中的ip导致上下游耦合】**

![图片](php_实践.assets/640-20211008120138442)

即“缘起”中举的例子，==下游服务换ip，可能导致多个服务调用方修改配置重启==。上下游间接的通过ip这个配置耦合在了一起，架构不合理。

**优化方案：通过内网域名而不是ip来进行下游连接**

![图片](php_实践.assets/640-20211008120214352)

如果在配置中使用==内网域名来进行下游连接==，当下游服务或者数据库更换ip时，只需要**运维层面将内网域名指向新的ip，然后统一切断原有旧的连接**，连接就能够==自动切换==到新的ip上来。这个过程不需要所有上游配合，非常帅气，强烈推荐！



**【case5：下游扩容导致上下游耦合】**

![图片](php_实践.assets/640-20211008120336128)

这次不是换换ip这么简单了，下游服务提供方原来是集群（ip1/ip2/ip3，当然，上游配置的是内网域名），现在==集群要扩容为（ip1/ip2/ip3/ip4/ip5）==，如果没有特殊的架构设计，上游往往需要修改配置，新增扩容后的节点，再重启，导致上下游耦合。



##### 4、总结

**如何发现系统架构中不合理的“反向依赖”设计？**

回答：

（1）变动方是A，配合方却是BCDE

（2）需求方是A，改动方确是BCDE

想想“换IP的是你，配合重启的却是我”，此时往往架构上可以进行解耦优化。

 

**常见反向依赖及优化方案？**

（1）==公共库导致耦合==

**优化一**：如果公共库是==业务特性代码==，进行公共库**垂直拆分**

**优化二**：如果公共库是==业务共性代码==，进行**服务化**下沉抽象



（2）==服务化不彻底导致耦合==

**特征**：服务中包==含大量“根据不同业务，执行不同个性分支”的代码==

**优化方案**：个性代码放到业务层实现，**将服务化更彻底更纯粹**



（3）==notify的不合理实现导致的耦合==

**特征**：调用方不关注执行结果，==以调用的方式去实现通知==，新增订阅者，修改代码的是发布者

**优化方案**：**通过MQ解耦**



（4）==配置中的ip导致上下游耦合==

**特征**：==多个上游需要修改配置重启==

**优化方案**：**使用内网域名替代内网ip**，通过“修改DNS指向，统一切断旧连接”的方式来上游无感切换



（5）==下游扩容导致上下游耦合==

特性：==多个上游需要修改配置重启==



#### 3.互联网架构如何实现“高可用”

##### 1、什么是高可用？

**高可用HA****（**High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，==通过设计减少系统不能提供服务的时间==。

假设系统一直能够提供服务，我们说系统的可用性是100%。

如果系统每运行100个时间单位，会有1个时间单位无法提供服务，我们说系统的可用性是99%。

很多公司的高可用目标是4个9，也就是99.99%，这就意味着，系统的年停机时间为8.76个小时。



##### 2、如何保障系统的高可用?

我们都知道，单点是系统高可用的大敌，单点往往是系统高可用最大的风险和敌人，应该尽量在系统设计的过程中避免单点。==方法论上，高可用保证的原则是“集群化”，或者叫“冗余”==：只有一个单点，挂了服务会受影响；如果有冗余备份，挂了还有其他backup能够顶上。

**保证系统高可用，架构设计的核心准则是：冗余。**

有了冗余之后，还不够，每次出现故障需要人工介入恢复势必会增加系统的不可服务实践。所以，又往往是==通过“自动故障转移”来实现系统的高可用==。

接下来我们看下典型互联网架构中，如何通过**冗余+自动故障转移**来保证系统的高可用特性。



##### 3、常见的互联网分层架构

![图片](php_实践.assets/640-20211008121134586)

常见互联网分布式架构如上，分为：

（1）**客户端层**：典型调用方是浏览器browser或者手机应用APP

（2）**反向代理层**：系统入口，反向代理

（3）**站点应用层**：实现核心应用逻辑，返回html或者json

（4）**服务层**：如果实现了服务化，就有这一层

（5）**数据****-****缓存层**：缓存加速访问存储

（6）**数据****-****数据库层**：数据库固化数据存储

整个系统的高可用，又是通过每一层的**冗余****+****自动故障转移**来综合实现的。



##### 4、分层高可用架构实践

**【客户端层->反向代理层】的高可用**

![图片](php_实践.assets/640-20211008121228943)

【客户端层】到【反向代理层】的**高可用**，是通过反向代理层的冗余来实现的。以nginx为例：有两台nginx，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。

![图片](php_实践.assets/640-20211008121303564)

**自动故障转移**：当nginx挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-nginx，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。



**【反向代理层->站点层】的高可用**

![图片](php_实践.assets/640-20211008121333982)

【反向代理层】到【站点层】的**高可用**，是通过站点层的冗余来实现的。假设反向代理层是nginx，nginx.conf里能够配置多个web后端，并且nginx能够探测到多个后端的存活性。

![图片](php_实践.assets/640-20211008121348331)

**自动故障转移**：当web-server挂了的时候，nginx能够探测到，会自动的进行故障转移，将流量自动迁移到其他的web-server，整个过程由nginx自动完成，对调用方是透明的。



**【站点层->服务层】的高可用**

![图片](php_实践.assets/640-20211008121414657)

【站点层】到【服务层】的**高可用**，是通过服务层的冗余来实现的。“服务连接池”会建立与下游服务多个连接，每次请求会“随机”选取连接来访问下游服务。

![图片](php_实践.assets/640-20211008121501991)

**自动故障转移**：当service挂了的时候，service-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的service，整个过程由连接池自动完成，对调用方是透明的（所以说RPC-client中的服务连接池是很重要的基础组件）。



**【服务层>缓存层】的高可用**

![图片](php_实践.assets/640-20211008121527883)

【服务层】到【缓存层】的高可用，是通过缓存数据的冗余来实现的。

缓存层的数据冗余又有几种方式：第一种是利用客户端的封装，service对cache进行双读或者双写。

![图片](php_实践.assets/640-20211008121538581)

缓存层也可以通过支持主从同步的缓存集群来解决缓存层的**高可用**问题。

以redis为例，redis天然支持主从同步，redis官方也有sentinel哨兵机制，来做redis的存活性检测。

![图片](php_实践.assets/640-20211008121550838)

**自动故障转移**：当redis主挂了的时候，sentinel能够探测到，会通知调用方访问新的redis，整个过程由sentinel和redis集群配合完成，对调用方是透明的。

 

说完缓存的高可用，这里要多说一句，业务对缓存并不一定有“高可用”要求，==更多的对缓存的使用场景，是用来“加速数据访问”==：把一部分数据放到缓存里，如果缓存挂了或者缓存没有命中，是可以去后端的数据库中再取数据的。

这类允许“cache miss”的业务场景，缓存架构的建议是：

![图片](php_实践.assets/640-20211008121623602)

==将kv缓存封装成服务集群==，上游设置一个代理（代理可以用集群冗余的方式保证高可用），代理的后端根据缓存访问的key水平切分成若干个实例，每个实例的访问并不做高可用。

![图片](php_实践.assets/640-20211008121641041)

**缓存实例挂了屏蔽**：当有水平切分的实例挂掉时，代理层直接返回cache miss，此时缓存挂掉对调用方也是透明的。key水平切分实例减少，==不建议做re-hash，这样容易引发缓存数据的不一致==。



**【服务层>数据库层】的高可用**

大部分互联网技术，数据库层都用了“主从同步，读写分离”架构，所以数据库层的高可用，又分为==“读库高可用”与“写库高可用”==两类。



**【服务层>数据库层“读”】的高可用**

![图片](php_实践.assets/640-20211008121739263)

【服务层】到【数据库读】的**高可用**，是通过读库的冗余来实现的。

既然冗余了读库，一般来说就至少有2个从库，“数据库连接池”会建立与读库多个连接，每次请求会路由到这些读库。

![图片](php_实践.assets/640-20211008121750298)

**自动故障转移**：当读库挂了的时候，db-connection-pool能够探测到，会自动的进行故障转移，将流量自动迁移到其他的读库，整个过程由连接池自动完成，对调用方是透明的（所以说DAO中的数据库连接池是很重要的基础组件）。



**【服务层>数据库层“写”】的高可用**

![图片](php_实践.assets/640-20211008121813438)

【服务层】到【数据库写】的**高可用**，是通过写库的冗余来实现的。

以mysql为例，可以设置两个mysql双主同步，一台对线上提供服务，另一台冗余以保证高可用，常见的实践是keepalived存活探测，相同virtual IP提供服务。

![图片](php_实践.assets/640-20211008121825327)

**自动故障转移**：当写库挂了的时候，keepalived能够探测到，会自动的进行故障转移，将流量自动迁移到shadow-db-master，由于使用的是相同的virtual IP，这个切换过程对调用方是透明的。



##### 5、总结

高可用HA（High Availability）是分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计减少系统不能提供服务的时间。

方法论上，高可用是通过**冗余****+****自动故障转移**来实现的。

整个互联网分层系统架构的高可用，又是通过每一层的**冗余****+****自动故障转移**来综合实现的，具体的：

（1）【客户端层】到【反向代理层】的高可用，是==通过反向代理层的冗余实现的==，常见实践是keepalived + virtual IP自动故障转移

（2）【反向代理层】到【站点层】的高可用，是==通过站点层的冗余实现的==，常见实践是nginx与web-server之间的存活性探测与自动故障转移

（3）【站点层】到【服务层】的高可用，是==通过服务层的冗余实现的==，常见实践是通过service-connection-pool来保证自动故障转移

（4）【服务层】到【缓存层】的高可用，是==通过缓存数据的冗余实现的==，常见实践是缓存客户端双读双写，或者利用缓存集群的主从数据同步与sentinel保活与自动故障转移；更多的业务场景，对缓存没有高可用要求，可以使用缓存服务化来对调用方屏蔽底层复杂性

（5）【服务层】到【数据库“读”】的高可用，是==通过读库的冗余实现的==，常见实践是通过db-connection-pool来保证自动故障转移

（6）【服务层】到【数据库“写”】的高可用，是==通过写库的冗余实现的==，常见实践是keepalived + virtual IP自动故障转移



#### 4.互联网架构如何实现“高并发”

##### 1、什么是高并发？

**高并发（**High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，==通过设计保证系统能够同时并行处理很多请求==。



高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。



**响应时间**：系统对请求做出响应的时间。例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。

**吞吐量**：单位时间内处理的请求数量。

**QPS**：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有这么明显。

**并发用户数**：同时承载正常使用系统功能的用户数量。例如一个即时通讯系统，同时在线量一定程度上代表了系统的并发用户数。



##### 2、如何提升系统的并发能力？

互联网分布式架构设计，==提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）==。

**垂直扩展**：提升单机处理能力。垂直扩展的方式又有两种：

（1）增强单机硬件性能，例如：增加CPU核数如32核，升级更好的网卡如万兆，升级更好的硬盘如SSD，扩充硬盘容量如2T，扩充系统内存如128G；

（2）提升单机架构性能，例如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间；



在互联网业务发展非常迅猛的早期，如果预算不是问题，强烈建议使用“增强单机硬件性能”的方式提升系统并发能力，因为这个阶段，公司的战略往往是发展业务抢时间，而“增强单机硬件性能”往往是最快的方法。

 

不管是提升单机硬件性能，还是提升单机架构性能，都有一个致命的不足：单机性能总是有极限的。所以==互联网分布式架构设计高并发终极解决方案还是水平扩展==。



**水平扩展**：只要增加服务器数量，就能线性扩充系统性能。水平扩展对系统架构设计是有要求的，如何在架构各层进行可水平扩展的设计，以及互联网公司架构各层常见的水平扩展实践，是本文重点讨论的内容。



##### 3、常见的互联网分层架构

![图片](php_实践.assets/640-20211008122446841)

常见互联网分布式架构如上，分为：

（1）**客户端层**：典型调用方是浏览器browser或者手机应用APP

（2）**反向代理层**：系统入口，反向代理

（3）**站点应用层**：实现核心应用逻辑，返回html或者json

（4）**服务层**：如果实现了服务化，就有这一层

（5）**数据-缓存层**：缓存加速访问存储

（6）**数据-数据库层**：数据库固化数据存储

整个系统各层次的水平扩展，又分别是如何实施的呢？



##### 4、分层水平扩展架构实践

**反向代理层的水平扩展**

![图片](php_实践.assets/640-20211008122559746)

反向代理层的水平扩展，是通过“DNS轮询”实现的：dns-server对于一个域名配置了多个解析ip，每次DNS解析请求来访问dns-server，会轮询返回这些ip。

当nginx成为瓶颈的时候，只要增加服务器数量，新增nginx服务的部署，增加一个外网ip，就能扩展反向代理层的性能，做到理论上的无限高并发。

**站点层的水平扩展**

![图片](php_实践.assets/640-20211008122618689)

站点层的水平扩展，是通过“nginx”实现的。通过修改nginx.conf，可以设置多个web后端。

当web后端成为瓶颈的时候，只要增加服务器数量，新增web服务的部署，在nginx配置中配置上新的web后端，就能扩展站点层的性能，做到理论上的无限高并发。



**服务层的水平扩展**

![图片](php_实践.assets/640-20211008122636470)

服务层的水平扩展，是通过“服务连接池”实现的。

站点层通过RPC-client调用下游的服务层RPC-server时，RPC-client中的连接池会建立与下游服务多个连接，当服务成为瓶颈的时候，只要增加服务器数量，新增服务部署，在RPC-client处建立新的下游服务连接，就能扩展服务层性能，做到理论上的无限高并发。如果需要优雅的进行服务层自动扩容，这里可能需要配置中心里服务自动发现功能的支持。



**数据层的水平扩展**

在数据量很大的情况下，数据层（缓存，数据库）涉及数据的水平扩展，将原本存储在一台服务器上的数据（缓存，数据库）水平拆分到不同服务器上去，以达到扩充系统性能的目的。

 

互联网数据层常见的水平拆分方式有这么几种，以数据库为例：

**按照范围水平拆分**

![图片](php_实践.assets/640-20211008122655542)

每一个数据服务，存储一定范围的数据，上图为例：

user0库，存储uid范围1-1kw

user1库，存储uid范围1kw-2kw

这个方案的好处是：

（1）规则简单，service只需判断一下uid范围就能路由到对应的存储服务；

（2）数据均衡性较好；

（3）比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务；

不足是：

（1）   请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大；



**按照哈希水平拆分**

![图片](php_实践.assets/640-20211008122718088)

每一个数据库，存储某个key值hash后的部分数据，上图为例：

user0库，存储偶数uid数据

user1库，存储奇数uid数据

这个方案的好处是：

（1）规则简单，service只需对uid进行hash能路由到对应的存储服务；

（2）数据均衡性较好；

（3）请求均匀性较好；

不足是：

（1）不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移；

 

这里需要注意的是，通过水平拆分来扩充系统性能，与主从同步读写分离来扩充数据库性能的方式有本质的不同。

通过水平拆分扩展数据库性能：

（1）每个服务器上存储的数据量是总量的1/n，所以单机的性能也会有提升；

（2）n个服务器上的数据没有交集，那个服务器上数据的并集是数据的全集；

（3）数据水平拆分到了n个服务器上，理论上读性能扩充了n倍，写性能也扩充了n倍（其实远不止n倍，因为单机的数据量变为了原来的1/n）；

通过主从同步读写分离扩展数据库性能：

（1）每个服务器上存储的数据量是和总量相同；

（2）n个服务器上的数据都一样，都是全集；

（3）理论上读性能扩充了n倍，写仍然是单点，写性能不变；

 

缓存层的水平拆分和数据库层的水平拆分类似，也是以范围拆分和哈希拆分的方式居多，就不再展开。



##### 5、总结

**高并发（**High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，==通过设计保证系统能够同时并行处理很多请求==。

==提高系统并发能力的方式，方法论上主要有两种：垂直扩展（Scale Up）与水平扩展（Scale Out）==。前者垂直扩展可以通过提升单机硬件性能，或者提升单机架构性能，来提高并发性，但单机性能总是有极限的，==互联网分布式架构设计高并发终极解决方案还是后者：水平扩展==。

互联网分层架构中，各层次水平扩展的实践又有所不同：

（1）反向代理层可以通过“DNS轮询”的方式来进行水平扩展；

（2）站点层可以通过nginx来进行水平扩展；

（3）服务层可以通过服务连接池来进行水平扩展；

（4）数据库可以按照数据范围，或者数据哈希的方式来进行水平扩展；

各层实施水平扩展后，能够通过增加服务器数量的方式来提升系统的性能，做到理论上的性能无限。



#### 5.典型数据库架构设计与实践

##### 1、用户中心

**用户中心**是一个常见业务，主要提供用户注册、登录、信息查询与修改的服务，其核心元数据为：

User(uid, uname, passwd, sex, age,nickname, …)

其中：

- uid为用户ID，主键
- uname, passwd, sex, age, nickname, …等为用户的属性

数据库设计上，一般来说在业务初期，单库单表就能够搞定这个需求。



##### 2、图示说明

为了方便大家理解，后文图片说明较多，其中：

- “灰色”方框，表示service，服务
- “紫色”圆框，标识master，主库
- “粉色”圆框，表示slave，从库



##### 3、单库架构

![图片](php_实践.assets/640-20211008152141978)

最常见的架构设计如上：

- user-service：用户中心服务，对调用者提供友好的RPC接口
- user-db：一个库进行数据存储

##### 4、分组架构

![图片](php_实践.assets/640-20211008152214476)

**什么是分组？**

**答**：分组架构是最常见的==一主多从，主从同步，读写分离==数据库架构：

- user-service：依旧是用户中心服务
- user-db-M(master)：主库，提供数据库写服务
- user-db-S(slave)：从库，提供数据库读服务

==主和从构成的数据库集群称为“组”==。

**分组有什么特点？**

**答**：同一个组里的数据库集群：

- 主从之间通过binlog进行数据同步
- 多个实例数据库结构完全相同
- 多个实例存储的数据也完全相同，本质上是将数据进行复制



**分组架构究竟解决什么问题？**

**答**：==大部分互联网业务读多写少==，**数据库的读往往最先成为性能瓶颈**，如果希望：

- 线性提升数据库读性能
- 通过消除读写锁冲突提升数据库写性能
- 通过冗余从库实现数据的“读高可用”

此时可以使用分组架构，需要注意的是，==分组架构中，数据库的主库依然是写单点==。

一句话总结，**分组解决的是“数据库读写高并发量高”问题**，所实施的架构设计。



##### 5、分片架构

![图片](php_实践.assets/640-20211008152413407)

**什么是分片？**

**答**：分片架构是大伙常说的==水平切分==(sharding)数据库架构：

- user-service：依旧是用户中心服务
- user-db1：水平切分成2份中的第一份
- user-db2：水平切分成2份中的第二份

分片后，多个数据库实例也会构成一个数据库集群。



**水平切分，到底是分库还是分表？**

**答**：==强烈建议分库==，而不是分表，因为：

- 分表依然公用一个数据库文件，仍然有磁盘IO的竞争
- 分库能够很容易的将数据迁移到不同数据库实例，甚至数据库机器上，扩展性更好



**水平切分，用什么算法？**

**答**：常见的水平切分算法有“==范围法==”和“==哈希法==”：

![图片](php_实践.assets/640-20211008152520653)

**范围法**如上图：以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

- user-db1：存储0到1千万的uid数据
- user-db2：存储0到2千万的uid数据

![图片](php_实践.assets/640-20211008152551875)

**哈希法**如上图：也是以用户中心的业务主键uid为划分依据，将数据水平切分到两个数据库实例上去：

- user-db1：存储uid取模得1的uid数据
- user-db2：存储uid取模得0的uid数据

这两种方法在互联网都有使用，其中哈希法使用较为广泛。



**分片有什么特点？**

**答**：同一个分片里的数据库集群：

- 多个实例之间本身不直接产生联系，不像主从间有binlog同步
- 多个实例数据库结构，也完全相同
- 多个实例存储的数据之间没有交集，所有实例间数据并集构成全局数据

 

**分片架构究竟解决什么问题？**

**答**：大部分互联网业务数据量很大，单库容量容易成为瓶颈，此时通过分片可以：

- 线性提升数据库写性能，需要注意的是，分组架构是不能线性提升数据库写性能的
- 降低单库数据容量

 

一句话总结，**分片解决的是“数据库数据量大”问题**，所实施的架构设计。



##### 6、分组+分片架构

![图片](php_实践.assets/640-20211008152742769)

如果业务**读写并发量很高，数据量也很大**，通常需要实施==分组+分片==的数据库架构：

- 通过分片来降低单库的数据量，线性提升数据库的写性能
- 通过分组来线性提升数据库的读性能，保证读库的高可用



##### 7、垂直切分

除了水平切分，垂直切分也是一类常见的数据库架构设计，==垂直切分一般和业务结合比较紧密==。

![图片](php_实践.assets/640-20211008152843396)

还是以用户中心为例，可以这么进行垂直切分：

User(uid, uname, passwd, sex, age, …)

User_EX(uid, intro, sign, …)

- 垂直切分开的表，主键都是uid
- 登录名，密码，性别，年龄等属性放在一个垂直表（库）里
- 自我介绍，个人签名等属性放在另一个垂直表（库）里



**如何进行垂直切分？**

**答**：根据业务对数据进行垂直切分时，一般要考虑属性的“==长度==”和“==访问频度==”两个因素：

- 长度较短，访问频率较高的放在一起
- 长度较长，访问频度较低的放在一起

这是因为，数据库会以行(row)为单位，将数load到内存(buffer)里，在内存容量有限的情况下，长度短且访问频度高的属性，内存能够load更多的数据，命中率会更高，磁盘IO会减少，数据库的性能会提升。

 

**垂直切分有什么特点？**

**答**：垂直切分和水平切有相似的地方，又不太相同：

- 多个实例之间也不直接产生联系，即没有binlog同步
- 多个实例数据库结构，都不一样
- 多个实例存储的数据之间至少有一列交集，一般来说是业务主键，所有实例间数据并集构成全局数据



**垂直切分解决什么问题？**

**答**：垂直切分即可以==降低单库的数据量==，还可以==降低磁盘IO从而提升吞吐量==，但它与业务结合比较紧密，并不是所有业务都能够进行垂直切分的。

 

##### 8、总结

文章较长，希望至少记住这么几点：

- 业务初期用==单库==
- 读压力大，读高可用，用==分组==
- 数据量大，写线性扩容，用==分片==
- 属性短，访问频度高的属性，==垂直拆分==到一起



#### 6.session一致性架构设计实践

##### 1、缘起

**什么是session？**

服务器为每个用户创建一个会话，存储用户的相关信息，以便多次请求能够定位到同一个上下文。

Web开发中，web-server可以自动为同一个浏览器的访问用户自动创建session，提供数据存储功能。最常见的，会把用户的登录信息、用户信息存储在session中，以保持登录状态。



**什么是session一致性问题？**

只要用户不重启浏览器，每次http短连接请求，理论上服务端都能定位到session，保持会话。

![图片](php_实践.assets/640-20211008154206247)

当只有一台web-server提供服务时，每次http短连接请求，都能够正确路由到存储session的对应web-server（废话，因为只有一台）。

此时的web-server是无法保证高可用的，采用“冗余+故障转移”的==多台web-server来保证高可用时，每次http短连接请求就不一定能路由到正确的session了==。

![图片](php_实践.assets/640-20211008154240664)

如上图，假设用户包含登录信息的session都记录在第一台web-server上，反向代理如果将请求路由到另一台web-server上，可能就找不到相关信息，而导致用户需要重新登录。 

在web-server高可用时，如何保证session路由的一致性，是今天将要讨论的问题。



##### 2、session同步法

![图片](php_实践.assets/640-20211008154326365)

**思路**：==多个web-server之间相互同步session==，这样每个web-server之间都包含全部的session

 

**优点**：web-server支持的功能，==应用程序不需要修改代码==

 

**不足**：

- session的同步需要数据传输，==占**内网带宽**==，有时延
- 所有web-server都包含所有session数据，数据量==受内存限制，无法水平扩展==
- ==有更多web-server时要歇菜==



##### 3、客户端存储法

![图片](php_实践.assets/640-20211008154443158)

**思路**：服务端存储所有用户的session，内存占用较大，可以将==session存储到浏览器cookie中==，每个端只要存储一个用户的数据了

**优点**：==服务端不需要存储==

**缺点**：

- 每次http请求都携带session，==占**外网带宽**==
- 数据存储在端上，并在网络传输，存在泄漏、篡改、窃取等==安全隐患==
- session==存储的数据大小受cookie限制==

“端存储”的方案虽然不常用，但确实是一种思路。



##### 3、反向代理hash一致性

**思路**：web-server为了保证高可用，有多台冗余，反向代理层能不能做一些事情，让==同一个用户的请求保证落在一台web-server上==呢？

![图片](php_实践.assets/640-20211008154649175)

**方案一：四层代理hash**

反向代理层使用==用户ip来做hash==，以保证同一个ip的请求落在同一个web-server上

![图片](php_实践.assets/640-20211008154707357)



**方案二：七层代理hash**

反向代理使用http协议中的==某些业务属性来做hash==，例如sid，city_id，user_id等，能够更加灵活的实施hash策略，以保证同一个浏览器用户的请求落在同一个web-server上

**优点**：

- 只需要改nginx配置，==不需要修改应用代码==
- ==负载均衡==，只要hash属性是均匀的，多台web-server的负载是均衡的
- 可以==支持web-server水平扩展==（session同步法是不行的，受内存限制）

 

**不足**：

- ==如果web-server重启，一部分session会丢失==，产生业务影响，例如部分用户重新登录
- 如果web-server水平扩展，==rehash后session重新分布，也会有一部分用户路由不到正确的session==

 

session一般是有有效期的，所有不足中的两点，可以认为等同于部分session失效，一般问题不大。

 

==对于四层hash还是七层hash，个人推荐前者==：**让专业的软件做专业的事情**，反向代理就负责转发，尽量不要引入应用层业务属性，除非不得不这么做（例如，有时候多机房多活需要按照业务属性路由到不同机房的web-server）。



##### 4、后端统一存储

![图片](php_实践.assets/640-20211008154934537)

**思路**：==将session存储在web-server后端的存储层==，数据库或者缓存

 

**优点**：

- ==没有安全隐患==
- ==可以水平扩展==，数据库/缓存水平切分即可
- web-server重启或者扩容都==不会有session丢失==

 

**不足**：增加了一次网络调用，并且==需要修改应用代码==

 

==对于db存储还是cache，个人推荐后者==：session读取的频率会很高，数据库压力会比较大。如果有session高可用需求，cache可以做高可用，但大部分情况下session可以丢失，一般也不需要考虑高可用。



##### 5、总结

**保证session一致性**的架构设计常见方法：

- ==session同步法==：多台web-server相互同步数据
- ==客户端存储法==：一个用户只存储自己的数据
- ==反向代理hash一致性==：四层hash和七层hash都可以做，保证一个用户的请求落在一台web-server上
- ==后端统一存储==：web-server重启和扩容，session也不会丢失

 

==对于方案3和方案4，个人建议推荐后者==：

- **web层、service层无状态是大规模分布式系统设计原则之一**，session属于状态，不宜放在web层
- **让专业的软件做专业的事情**，web-server存session？还是让cache去做这样的事情吧



#### 7.TCP接入层的负载均衡、高可用、扩展性架构

##### 1、web-server的负载均衡

![图片](php_实践.assets/640-20211008155433641)

互联网架构中，==web-server接入一般使用nginx来做反向代理==，实施负载均衡。整个架构分三层：

- **上游调用层**，一般是browser或者APP
- **中间反向代理层**，nginx
- **下游真实接入集群**，web-server，常见web-server的有tomcat，apache

 

整个访问过程为：

- browser向daojia.com发起请求
- DNS服务器将daojia.com解析为==外网IP==(1.2.3.4)
- browser通过外网IP(1.2.3.4)访问nginx
- nginx实施负载均衡策略，常见策略有轮询，随机，IP-hash等
- nginx将请求转发给==内网IP==(192.168.0.1)的web-server

由于==http短连接，以及web应用无状态==的特性，理论上**任何一个http请求落在任意一台web-server都应该得到正常处理**（如果必须落在一台，说明架构不合理，不能水平扩展）。

问题来了，==tcp是有状态的连接==，客户端和服务端一旦建立连接，**一个client发起的请求必须落在同一台tcp-server上**，此时如何做负载均衡，如何保证水平扩展呢？



##### 2、单机法tcp-server

![图片](php_实践.assets/640-20211008155629720)

单个tcp-server显然是可以保证请求一致性：

- client向tcp.daojia.com发起tcp请求
- DNS服务器将tcp.daojia.com解析为外网IP(1.2.3.4)
- client通过外网IP(1.2.3.4)向tcp-server发起请求

 

**方案的缺点？**

==无法保证高可用。==



##### 3、集群法tcp-server

![图片](php_实践.assets/640-20211008155707691)

==通过搭建tcp-server集群来保证高可用==，**客户端来实现负载均衡**：

- client内配置有tcp1/tcp2/tcp3.daojia.com三个tcp-server的外网IP
- 客户端通过“随机”的方式选择tcp-server，假设选择到的是tcp1.daojia.com
- 通过DNS解析tcp1.daojia.com
- 通过外网IP连接真实的tcp-server



**如何保证高可用呢？**

==如果client发现某个tcp-server连接不上，则选择另一个。==

 

**潜在的缺点？**

每次连接前，需要多实施一次DNS访问：

- ==难以预防DNS劫持==
- 多一次DNS访问意味着==更长的连接时间==，这个不足在手机端更为明显

 

**如何解决DNS的问题？**

直接将IP配置在客户端，可以解决上述两个问题，很多公司也就是这么做的（俗称“**IP直通车**”）。

 

**“IP直通车”有什么新问题？**

将IP写死在客户端，在客户端实施负载均衡，==扩展性很差==：

- ==如果原有IP发生变化==，客户端得不到实时通知
- ==如果新增IP==，即tcp-sever扩容，客户端也得不到实时通知
- ==如果负载均衡策略变化==，需要升级客户端



##### 4、服务端实施负载均衡

==只有将复杂的策略下沉到服务端，才能根本上解决扩展性的问题。==

![图片](php_实践.assets/640-20211008155848556)

==增加一个http接口，将客户端的“IP配置”与“均衡策略”放到服务端==是一个不错的方案：

- client每次访问tcp-server前，先调用一个**新增的get-tcp-ip接口**，对于client而言，==这个http接口只返回一个tcp-server的IP==
- 这个http接口，实现的是原client的IP均衡策略
- 拿到tcp-server的IP后，和原来一样向tcp-server发起TCP长连接

 

这样的话，扩展性问题就解决了：

- 如果原有IP发生变化，只需要修改get-tcp-ip接口的配置
- 如果新增IP，也是修改get-tcp-ip接口的配置
- 如果负载均衡策略变化，需要升级客户端

 

然而，**新的问题**又产生了，如果所有IP放在客户端，当有一个IP挂掉的时候，client可以再换一个IP连接，保证可用性，而get-tcp-ip接口只是维护静态的tcp-server集群IP，对于这些==IP对应的tcp-server是否可用==，是完全不知情的，怎么办呢？



##### 5、tcp-server状态上报

![图片](php_实践.assets/640-20211008155954314)

get-tcp-ip接口怎么知道tcp-server集群中各台服务器是否可用呢，==tcp-server主动上报是一个潜在方案==，如果某一个tcp-server挂了，则会终止上报，对于停止上报状态的tcp-server，get-tcp-ip接口，将不返回给client相应的tcp-server的外网IP。

 

**该设计的存在的问题？**

诚然，状态上报解决了tcp-server高可用的问题，但这个设计犯了一个==“反向依赖”的耦合小错误==：使得tcp-server要依赖于一个与本身业务无关的web-server。



##### 6、tcp-server状态拉取

![图片](php_实践.assets/640-20211008160046539)

**更优的方案**是：==web-server通过“拉”的方式获取各个tcp-server的状态==，而不是tcp-server通过“推”的方式上报自己的状态。

 

这样的话，==每个tcp-server都独立与解耦==，只需专注于资深的tcp业务功能即可。



**高可用、负载均衡、扩展性**等任务==由get-tcp-ip的web-server专注来执行==。

 

多说一句，==将负载均衡实现在服务端==，还有一个**好处**，可以实现==异构tcp-server的负载均衡==，以及==过载保护==：

- **静态实施**：web-server下的多个tcp-server的IP可以配置负载权重，根据tcp-server的机器配置分配负载（nginx也有类似的功能）
- **动态实施**：web-server可以根据“拉”回来的tcp-server的状态，动态分配负载，并在tcp-server性能极具下降时实施过载保护



##### 7、总结

**web-server如何实施负载均衡？**

==利用nginx反向代理来轮询、随机、ip-hash。==

 

**tcp-server怎么快速保证请求一致性？**

==单机。==

 

**如何保证高可用？**

==客户配置多个tcp-server的域名。==

 

**如何防止DNS劫持，以及加速？**

==IP直通车，客户端配置多个tcp-server的IP。==

 

**如何保证扩展性？**

==服务端提供get-tcp-ip接口==，向client屏屏蔽负载均衡策略，并实施便捷扩容。

 

**如何保证高可用？**

==tcp-server“推”==状态给get-tcp-ip接口，

or

==get-tcp-ip接口“拉”==tcp-server状态。





#### 8.配置中心架构设计演进

##### 1、缘起

随着互联网业务的越来越复杂，用户量与流量越来越大，==“服务化分层”是架构演进的必由之路==。

https://www.jianshu.com/p/a476bdee4db1